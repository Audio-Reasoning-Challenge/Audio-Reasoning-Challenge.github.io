---
layout: page
title: Organizers
permalink: /organizers/
---

<style>
.organizers-container {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(450px, 1fr));
  gap: 30px;
  margin: 30px 0;
}

.organizer-card {
  padding: 25px;
  background: #f8f9fa;
  border-radius: 8px;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.organizer-card h3 {
  margin-top: 0;
  color: #2c3e50;
  font-size: 1.3em;
  margin-bottom: 15px;
}

.organizer-card p {
  line-height: 1.6;
  color: #555;
  text-align: justify;
  margin-bottom: 10px;
}

.organizer-email {
  color: #0066cc;
  font-family: monospace;
  word-break: break-all;
}

@media (max-width: 768px) {
  .organizers-container {
    grid-template-columns: 1fr;
  }
}
</style>

<div class="organizers-container">

<div class="organizer-card">
  <h3>Ziyang Ma</h3>
  <p>Ziyang Ma is currently within the Joint Ph.D. Program, co-supervised by Prof. Xie Chen from Shanghai Jiao Tong University and Prof. Eng-Siong Chng from Nanyang Technological University. His research focuses on speech and audio processing, including SSL, LLM, reasoning, alignment, and more. He has published 10 first-author papers at top machine learning and speech conferences, with over 2,000 citations. He was awarded the Interspeech 2023 Best Student Paper Shortlist, Nanyang Speech Technology Forum 2025 Best Student Paper Award, etc. He leads the open-source emotion2vec series, SLAM-LLM series, and a series of speech and audio reasoning work.</p>
  <p><strong>Email:</strong> <span class="organizer-email">zym.22@sjtu.edu.cn</span></p>
</div>

<div class="organizer-card">
  <h3>Yinghao Ma</h3>
  <p>Yinghao Ma is a Ph.D. candidate at the Centre for Digital Music (C4DM), Queen Mary University of London, working on Large Language Models (LLMs) for Music and multimodal audio reasoning. His research spans audio-language modeling, benchmark design, and interactive music generation systems. He is the recipient of the Google Computer Science PhD Fellowship 2025 and the co-founder of the MAP (Music, Audio, and Perception) research community, which promotes open, interdisciplinary collaboration in LLM research. He has led the development of several influential multimodal benchmarks for music, and aims to bridge speech, sound, and music within unified reasoning-capable AI systems.</p>
  <p><strong>Email:</strong> <span class="organizer-email">yinghao.ma@qmul.ac.uk</span></p>
</div>

<div class="organizer-card">
  <h3>Chao-Han Huck Yang</h3>
  <p>Chao-Han Huck Yang is a senior research scientist at NVIDIA Research. He received his Ph.D. and M.Sc. from Georgia Institute of Technology, USA and B.Sc. from National Taiwan University. His primary research lies in the area of speech-language modeling, robust speech recognition, and multi-modal post-training alignments. He served as area and senior chairs and committee members in IEEE ICASSP 2022 to 2025, EMNLP 2024, SLT 2024, and NAACL 2025. He has served in the IEEE SPS technical committee at Applied Signals Processing Systems (ASPS) and Data Collection Committee (DCC) since 2022.</p>
  <p><strong>Email:</strong> <span class="organizer-email">hucky@nvidia.com</span></p>
</div>

<div class="organizer-card">
  <h3>Ruiyang Xu</h3>
  <p>Ruiyang Xu is an undergraduate student at Shanghai Jiao Tong University, advised by Prof. Xie Chen. His research interests include audio and multimodal understanding, reasoning, and LLM. He is a core contributor to MMAR and also contributes to the development of the open-source SLAM-LLM series.</p>
  <p><strong>Email:</strong> <span class="organizer-email">xry2022@sjtu.edu.cn</span></p>
</div>

<div class="organizer-card">
  <h3>Bohan Li</h3>
  <p>Bohan Li received his B.Eng. in Computer Science in 2024 and is currently pursuing a Ph.D. degree at Shanghai Jiao Tong University under the supervision of Prof. Kai Yu. His research focuses on text-to-speech synthesis, speech and language processing, and efficient machine learning. He has published papers in top speech processing and AI conferences, including ICASSP, Interspeech and NeurIPS.</p>
  <p><strong>Email:</strong> <span class="organizer-email">everlastingnight@sjtu.edu.cn</span></p>
</div>

<div class="organizer-card">
  <h3>Jaeyeon Kim</h3>
  <p>Jaeyeon Kim is a Ph.D. student at the Language Technologies Institute, Carnegie Mellon University, advised by Prof. Carlos Busso and Prof. Shinji Watanabe. His research focuses on audio–multimodal learning, particularly on audio–language models and audio–visual learning. He has published papers at top conferences such as ICASSP, ICLR, and NeurIPS.</p>
  <p><strong>Email:</strong> <span class="organizer-email">jaeyeon2@andrew.cmu.edu</span></p>
</div>

<div class="organizer-card">
  <h3>Jin Xu</h3>
  <p>Jin Xu is currently leading the audio group at Qwen Team, Alibaba, responsible for research on audio understanding, real-time multimodal interaction, speech synthesis, general audio synthesis, and chat models centered on audio. Prior to this, He obtained the Ph.D. degree from the Institute for Interdisciplinary Information Sciences (IIIS) at Tsinghua University, supervised by Prof. Jian Li. Currently, He has published dozens of papers at top international AI conferences such as ICLR, ICML, NeurIPS, and KDD. His representative projects are Qwen2.5-Omni, Qwen2-Audio and Qwen-Audio.</p>
  <p><strong>Email:</strong> <span class="organizer-email">renjun.xj@alibaba-inc.com</span></p>
</div>

<div class="organizer-card">
  <h3>Jinyu Li</h3>
  <p>Jinyu Li currently serves as a Partner Applied Science Manager at Microsoft, leading the science team dedicated to designing and enhancing speech modeling algorithms and technologies. He is an IEEE Fellow, for contributions to deep-learning-based speech technology innovation and commercialization. He has been a member of IEEE Speech and Language Processing Technical Committee from 2017 to 2023. He also served as the associate editor of IEEE/ACM Transactions on Audio, Speech and Language Processing from 2015 to 2020. He is named as Distinguished Industry Speakers for IEEE Signal Processing Society, 2025.</p>
  <p><strong>Email:</strong> <span class="organizer-email">jinyli@microsoft.com</span></p>
</div>

<div class="organizer-card">
  <h3>Carlos Busso</h3>
  <p>Carlos Busso is a Professor at Language Technologies Institute, Carnegie Mellon University, where he is also the director of the Multimodal Speech Processing (MSP) Laboratory. Before joining CMU, he was a faculty member at The University of Texas at Dallas (UTD), where he served as an Assistant Professor (2009-2015), Associate Professor (2015-2020), and Full Professor (2020-2024) in the Department of Electrical and Computer Engineering. He is an IEEE Fellow and an ISCA Fellow. His research focuses on human-centered multimodal machine intelligence and its applications in speech-based interfaces.</p>
  <p><strong>Email:</strong> <span class="organizer-email">cbusso@andrew.cmu.edu</span></p>
</div>

<div class="organizer-card">
  <h3>Kai Yu</h3>
  <p>Kai Yu received the B.Eng. and the M.Sc. degrees from Tsinghua University, Beijing, China, in 1999 and 2002 respectively, and the Ph.D. degree from Cambridge University, U.K. in 2006. He is currently a distinguished Professor and the director of the Machine Intelligence Institute of School of Computer Science, SJTU. His research interests lie in the field of conversational AI, including rich aspects of speech and language processing as well as multi-modal linguistic computing. He is a fellow of the International Speech Communication Association (ISCA), an associate editor of the IEEE Transactions on Audio, Speech, and Language Processing (TASLP), and a senior IEEE member.</p>
  <p><strong>Email:</strong> <span class="organizer-email">kai.yu@sjtu.edu.cn</span></p>
</div>

<div class="organizer-card">
  <h3>Eng Siong Chng</h3>
  <p>Eng Siong Chng is currently a Professor of Computer Science in the College of Computing and Data Science, Nanyang Technological University, Singapore. He is also Asst Dean (Innovation and Industry) for CCDS, NTU. His main research interests are in speech, language processing, and Large Language Models. He is also the PI of AISG Speech Lab with an interest in developing robust code-switch ASR using LLM capabilities.</p>
  <p><strong>Email:</strong> <span class="organizer-email">aseschng@ntu.edu.sg</span></p>
</div>

<div class="organizer-card">
  <h3>Xie Chen</h3>
  <p>Xie Chen is currently an associate professor in the Computer Science School at Shanghai Jiao Tong University, China. He obtained his PhD degree in the information engineering department at Cambridge University. Before joining SJTU, he worked at Cambridge University as a research associate, and as a senior and principal researcher in the speech and language research group at Microsoft Research. His main research interest lies in deep learning, especially its application to speech processing, including speech recognition and synthesis.</p>
  <p><strong>Email:</strong> <span class="organizer-email">chenxie95@sjtu.edu.cn</span></p>
</div>

</div>